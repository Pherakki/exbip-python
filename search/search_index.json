{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#what-is-exbip-python","title":"What is exbip-python?","text":"<p><code>exbip-python</code> is a pure Python library of EXtensible BInary Parsers that aims to make data serialization fast and easy. The main features of <code>exbip</code> are:</p> <ul> <li>Serialization is (currently, see Future Development) performed verbosely with a single function that acts as a \"schema\", capable of handling deserialization, serialization, automatic data offset calculations, data validation, or other user-defined tasks, depending on an external object used to parse the data structure.</li> <li>It provides a simple interface to extend the library functionality with \"descriptors\" for binary data patterns not covered in the <code>exbip-python</code> \"standard library\".</li> </ul> <p>In addition, the library has been written with the goal of minimizing abstraction overhead in CPython. Some abstraction overhead is incurred by necessity in CPython, which has been partially mitigated by inlining function calls (by hand or by runtime monkey-patching), and optimizing out unnecessary attribute accesses.</p> <p>Further optimization and benchmarking work is always highly welcome.</p> <p>Note that <code>exbip</code> is a work-in-progress. There are likely areas that the API either does not fit as well as it could do, or is missing functionality that is useful for a large number of workflows. Issues to discuss this (followed by pull requests to fix them) are very much appecriated. However, <code>exbip</code> has been used successfully in several projects before its standalone release and is capable of building useable and validatable serialization logic.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<p>In the documentation you will find:</p> <ol> <li> <p>Getting Started: A gentle introduction to using <code>exbip</code> which covers the major features.</p> </li> <li> <p>Standard Library: An exhaustive list of every function and class in the exbip standard library that is exposed to the user.</p> </li> <li> <p>Framework: An exhaustive list of the framework functions and classes exposed to the user, which are used for creating extensions for <code>exbip</code>.</p> </li> <li> <p>Future Development: A list of topics under consideration for future development of the library.</p> </li> </ol>"},{"location":"02-gettingstarted/","title":"Getting Started","text":""},{"location":"02-gettingstarted/#installation","title":"Installation","text":"<p>Either install <code>exbip</code> from the package root directory with <code>pip install -e .</code>, or copy-and-paste the <code>exbip</code> folder from the repository into your repository as a sub-package.</p>"},{"location":"02-gettingstarted/#creating-a-serializable-structure","title":"Creating a Serializable Structure","text":"<p><code>exbip</code> comes equipped with a standard library designed to cover basic serialization needs. The parsing classes contained in the standard library are also a good base to extend the framework with user code from, so familiarity with the standard library is a good foundation for any use case.</p> <p>Due to Python's duck-typing, to create an <code>exbip</code>-compatible serializable we need only conform to the <code>exbip</code> serialization interface. This takes the form of equipping a class with an <code>exbip_rw</code> function that accepts a parser object as the first non-self argument, alongside arbitrary following arguments.</p> <pre><code>class MyStruct:\n    def __init__(self, a : int, b : int, c : float):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_uint16(self.b)\n        self.c = rw.rw_float32(self.c)\n</code></pre> <p>There are a few things requiring explanation here.</p> <ol> <li>The <code>rw</code> argument to <code>exbip_rw</code> is a parser object. This can be a file writer, a stream reader, or something else entirely that conforms (in abstract) to the parser interface the class is written against. We will see how to pass a valid <code>rw</code> into the struct in the following sections.</li> <li>The parser possesses a number of methods that are used to perform operations on data. In this example we are using the standard library functions <code>rw_uint32</code>, <code>rw_uint16</code>, and <code>rw_float32</code>, which respectively operate on an IEEE unsigned 32-bit integer, an IEEE unsigned 16 bit integer, and an IEEE 32-bit floating-point number. Note the language used here: these functions signify an arbitrary operation related to a particular binary data representation, with a parser providing a common theme behind these operations (such as reading or writing this data type from or to a stream).</li> <li>The syntax for <code>rw_uint32</code> etc. does look a bit odd: the variable in question is both passed into the function and assigned the result of the function. In short, this is to enable to interface to support both reading and writing: the input argument will be ignored for these functions when reading, and the output will be the same as the input when writing. This syntax is used because Python does not support reference or pointer types for immutable data types.</li> <li>We have not yet mentioned endianness. The standard library provides explicit little- and big-endian versions of these functions (e.g. <code>rw_uint32_le</code>, <code>rw_uint32_be</code>), and the ones we have invoked here have context-sensitive endianness. In other words, the endianness of <code>rw_uint32</code> is defined by the parser state. This will also be addressed fully in the section on serialization context.</li> </ol> <p>Now that we have a serializable object, we will see what we can do with it.</p>"},{"location":"02-gettingstarted/#writing-to-a-stream","title":"Writing to a Stream","text":"<p>We can serialize our class using the exbip <code>Writer</code> parser. We can do this in a few lines:</p> <pre><code>from exbip import Writer\n\nclass MyStruct:\n    def __init__(self, a : int, b : int, c : float):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_uint16(self.b)\n        self.c = rw.rw_float32(self.c)\n\ns = MyStruct(5, 1, 0.6)\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_obj(s)\n</code></pre> <p>A few explanatory notes are once more required.</p> <ol> <li>We are opening a file stream using the <code>FileIO()</code> method of the <code>Writer</code> class. There is a second option, <code>BytestreamIO()</code>, which will instead serialize to an <code>io.BytesIO()</code> stream.</li> <li>The stream held by the <code>Writer</code> is opened and closed by a context manager. All interactions with the stream must happen in this scope.</li> <li>We invoke the <code>exbip_rw</code> method of the struct by calling the <code>rw_obj</code> method of <code>Writer</code>. This is implemented as a simple wrapper function that calls the <code>exbip_rw</code> method of the given object. Note that since the received object is required to be mutable, there is no need for the interface to require assignment to <code>s</code> for this to work correctly.</li> <li>By default, the <code>Writer</code> serializes to little-endian data. We will address how to change this in the section on serialization context.</li> </ol> <p>To check that the file contains what we expect, we can just open it and read the contents using regular Python.</p> <p><pre><code>with open(\"test.bin\", 'rb') as FILE:\n    print(FILE.read(10))\n    # Check that we've read the whole file\n    assert FILE.read(1) == b''\n</code></pre> This should give you the bytestring <pre><code>&gt;&gt; b'\\x05\\x00\\x00\\x00\\x01\\x00\\x9a\\x99\\x19?'\n     |^^^^^^^^^^^^^^||^^^^^^||^^^^^^^^^^^|\n          5 (u32)    1 (u16)   0.6 (f32)\n</code></pre> as expected.</p>"},{"location":"02-gettingstarted/#reading-from-a-stream","title":"Reading from a Stream","text":"<p>Deserialization works very similarly to serialization; we need only swap out the <code>Writer</code> class for a <code>Reader</code> class and we can re-use the same logic.</p> <pre><code>from exbip import Reader\n\nclass MyStruct:\n    def __init__(self, a : int, b : int, c : float):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_uint16(self.b)\n        self.c = rw.rw_float32(self.c)\n\ns = MyStruct(None, None, None)\n\nwith Reader().FileIO(\"test.bin\") as rw:\n    rw.rw_obj(s)\n\nprint(s.a, s.b, s.c) # Prints \"5 1 0.6000000238418579\"\n</code></pre>"},{"location":"02-gettingstarted/#simplifying-with-traits","title":"Simplifying with Traits","text":"<p>Although it is no more than two lines, it is cumbersome to write out the entire context-manager scope for every object you wish to serialize. Moreover, converting a struct to a bytestring can be somewhat tedious, since it requires some extra lines of code:</p> <pre><code>with Writer().BytestreamIO() as rw:\n    rw.rw_obj(s)\n    rw._bytestream.seek(0)\n    s_bytes = rw._bytestream.read()\n</code></pre> <p>You could wrap this up into a method on the serializable class instead to encapsulate the boilerplate. <code>exbip</code> provides a number of \"trait\" mix-in classes that define member functions to automate this for you.</p> <p>The following snippet shows how to use these traits with your object. <pre><code>from exbip import Reader, ReadableTrait\nfrom exbip import Writer, WriteableTrait\n\nclass MyStruct(ReadableTrait(Reader), WriteableTrait(Writer)):\n    def __init__(self, a : int, b : int, c : float):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_uint16(self.b)\n        self.c = rw.rw_float32(self.c)\n</code></pre> You will see that these traits require a particular parser as an argument. This is so that if you extend the interface with custom data types, you can use these objects to provide the required set of operations without needing to implement the trait definitions yourself.</p> <p>Let's now see what these traits do for you. If you execute</p> <p><pre><code>print(dir(MyStruct))\n</code></pre> you will see that there are four new methods:</p> <ul> <li>read</li> <li>frombytes</li> <li>write</li> <li>tobytes</li> </ul> <p>The first two are defined by the <code>ReadableTrait</code>, and the latter two by the <code>WriteableTrait</code>. You can now avoid the boilerplate context-manager setup and initiate a (de)serialization process as</p> <pre><code>from exbip import Reader, ReadableTrait\nfrom exbip import Writer, WriteableTrait\n\nclass MyStruct(ReadableTrait(Reader), WriteableTrait(Writer)):\n    def __init__(self, a : int, b : int, c : float):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_uint16(self.b)\n        self.c = rw.rw_float32(self.c)\n\ns = MyStruct(None, None, None)\n\n# In all the below examples, the mandatory arguments\n# are given. Any additional arguments are passed to \n# the exbip_rw function on that class.\n\n# Read from file\ns.read(\"test.bin\")\n# Deserialize bytes into this object\ns.frombytes(b'\\x05\\x00\\x00\\x00\\x01\\x00\\x9a\\x99\\x19?')\n# Write to file\ns.write(\"test.bin\")\n# Serialize this object to bytes\nprint(s.tobytes())\n</code></pre> <p>If you have multiple objects that you might want to serialize independently of each other, you can bundle these traits into a base serializable class which your structs can then inherit from.</p> <pre><code>class MySerializable(ReadableTrait(Reader),\n                     WriteableTrait(Writer)):\n    pass\n\nclass MyStruct(MySerializable):\n    ...\n\nclass MyStruct2(MySerializable):\n    ...\n</code></pre>"},{"location":"02-gettingstarted/#objects","title":"Objects","text":"<p>We have already encountered the required function to read and write objects: <code>rw_obj</code>:</p> <pre><code>from exbip import Writer\n\nclass MyStruct:\n    def __init__(self, a, b):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\no = MyStruct(1, 0.5)\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_obj(o)\n</code></pre> <p>We can also read or write different objects by passing in a constructor to a diffrent function, <code>rw_dynamic_obj</code>:</p> <pre><code>from exbip import Reader, ReadableTrait\nfrom exbip import Writer, WriteableTrait\n\nclass MyStruct:\n    def __init__(self, a=None, b=None, c=None):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw, c):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n        self.c = c\n\nclass MyStruct2:\n    def __init__(self, a=None, b=None, c=None):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyBiggerStruct(ReadableTrait(Reader), WriteableTrait(Writer)):\n    def __init__(self):\n        self.obj  = None\n        self.obj2 = None\n\n    def exbip_rw(self, rw):\n        self.obj  = rw.rw_dynamic_obj(self.obj,  MyStruct, 10)\n        self.obj2 = rw.rw_dynamic_obj(self.obj2, lambda: MyStruct2(None, None, 10))\n\nb = MyBiggerStruct()\nb.obj = MyStruct(1, 0.2, None)\nb.obj2 = MyStruct2(2, 0.4, 10)\n\nb.write(\"test.bin\")\n\n# Look ma, no explicit construction!\nc = MyBiggerStruct()\nc.read(\"test.bin\")\n\n# 1 0.20000000298023224 10\nprint(c.obj.a, c.obj.b, c.obj.c)\n</code></pre> <p>Note that, unlike <code>rw_obj</code>, you must allocate the return value of <code>rw_dynamic_obj</code> to a variable, because it will return a completely new object when deserializing, rather than acting on a mutable object like <code>rw_obj</code> (because the object it acts on is created within the function call).</p> <p>There is also nothing stopping you from declaring additional functions that use a parser and calling them directly on your class:</p> <pre><code>from exbip import Writer\n\nclass MyStruct:\n    def __init__(self, a=None, b=None, c=None):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw, c):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n        self.c = c\n\nclass MyStruct2:\n    def __init__(self, a=None, b=None, c=None):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyBiggerStruct:\n    def __init__(self):\n        self.obj  = None\n        self.obj2 = None\n\n    def exbip_rw(self, rw):\n        self.obj  = rw.rw_dynamic_obj(self.obj,  MyStruct, 10)\n\n    def another_rw_function(self, rw):\n        self.obj2 = rw.rw_dynamic_obj(self.obj2, lambda: MyStruct2(10))\n\nb = MyBiggerStruct()\nb.obj = MyStruct(1, 0.2, None)\nb.obj2 = MyStruct2(2, 0.4, 10)\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_obj(b) # Invokes exbip_rw. We could also do b.exbip_rw(rw)\n    b.another_rw_function(rw) # Just calling a regular function on b\n</code></pre>"},{"location":"02-gettingstarted/#arrays","title":"Arrays","text":"<p>Clearly, arrays pose a bit of an issue with the current paradigm: if we're reading, the array hasn't yet been constructed, so we can't iterate through it and act on each element.</p> <p><code>exbip</code> provides two strategies to mitigate this: providing descriptors to read full arrays, and providing some \"iterator\" methods that construct elements during an iteration process. Both of these strategies are described in the following sections.</p>"},{"location":"02-gettingstarted/#primitive-arrays","title":"Primitive Arrays","text":"<p>You can read and write arrays of primitives with a similar set of functions to single primitives. These functions all share the convention of having the same name as their single counterparts, with an <code>s</code> attached to the end of the type name, and taking an additional <code>shape</code> argument that will transform the array into a nested list if it is more than one dimensional. A few examples:</p> <pre><code>from exbip import Writer\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_uint32s([1,2,3,4,5], shape=5)\n    rw.rw_float32s([(0.1,0.1,0.1), (0.2,0.2,0.2)], shape=(2, 3))\n</code></pre>"},{"location":"02-gettingstarted/#object-arrays","title":"Object Arrays","text":"<p>We've read one object so far, but what about a dynamic number of objects? We have two methods to help here, <code>rw_dynamic_objs</code> and <code>rw_dynamic_objs_while</code>:</p> <pre><code>from exbip import Writer\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nobjs = [MyStruct(1, 0.5), MyStruct(2, 1.0)]\nwith Writer().FileIO(\"test.bin\") as rw:\n    objs = rw.rw_dynamic_objs(objs, MyStruct, 2)\n</code></pre> <p>Providing the constructor and count may seem unnecessary in this example (and indeed, they are), but they are necessary for the deserialization implementation of this function call, since it needs to know what to construct and how many times to construct it.</p> <p>As for <code>rw_dynamic_objs_while</code>, this is a version of <code>rw_dynamic_objs</code> that uses a stopping condition rather than a count when deserializing:</p> <pre><code>from exbip import Reader, Writer\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nobjs = [MyStruct(1, 0.5), MyStruct(2, 1.0)]\nwith Writer().FileIO(\"test.bin\") as rw:\n    objs = rw.rw_dynamic_objs_while(objs, MyStruct, lambda rw: rw.tell() &lt; 0x10)\n\nwith Reader().FileIO(\"test.bin\") as rw:\n    objs2 = rw.rw_dynamic_objs_while(objs, MyStruct, lambda rw: rw.tell() &lt; 0x10)\n\n# 2 2\nprint(objs2[1].a, len(objs2))\n</code></pre> <p>For a <code>Reader</code>, the above script will continue reading objects as long as the stream position is less than <code>0x10</code>. Since each of our structs occupies 8 bytes, in this example it will stop after reading two structs.</p> <p>You can, of course, provide any boolean-valued callable as a stop condition. For complicated conditions, you may wish to provide a functor rather than a function or lambda:</p> <pre><code>from exbip import Reader, Writer\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyStopCondition:\n    def __init__(self, pos):\n        self.pos = pos\n\n    def __call__(self, rw):\n        return rw.tell() &lt; self.pos\n\nobjs = [MyStruct(1, 0.5), MyStruct(2, 1.0)]\nwith Writer().FileIO(\"test.bin\") as rw:\n    objs = rw.rw_dynamic_objs_while(objs, MyStruct, MyStopCondition(0x10))\n\nwith Reader().FileIO(\"test.bin\") as rw:\n    objs2 = rw.rw_dynamic_objs_while(None, MyStruct, MyStopCondition(0x10))\n\n# 2 2\nprint(objs2[1].a, len(objs2))\n</code></pre>"},{"location":"02-gettingstarted/#iterators","title":"Iterators","text":"<p>Sometimes you need to get even more complicated than a simple loop where each object is operated on in sequence. For these situations, <code>exbip</code> provides two \"iterators\": the <code>array_iterator</code> and <code>array_while_iterator</code>.</p> <p>These act like Python iterators.</p> <pre><code>from exbip import Writer, OffsetCalculator, OffsetMarker\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\n\nobjs = [MyStruct(1, 0.5), MyStruct(2, 1.0)]\nlst = []\ncount = len(objs)\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    for obj in rw.array_iterator(objs, MyStruct, count):\n        rw.rw_obj(obj)\n        # Contrived example...\n        lst.append(obj.a)\n\n# [1, 2]\nprint(lst)\n</code></pre> <p><code>array_while_iterator</code> works analogously to <code>rw_dynamic_objs_while</code>:</p> <pre><code>from exbip import Writer\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyStopCondition:\n    def __init__(self, pos):\n        self.pos = pos\n\n    def __call__(self, rw):\n        return rw.tell() &lt; self.pos\n\nobjs = [MyStruct(1, 0.5), MyStruct(2, 1.0)]\nlst = []\nwith Writer().FileIO(\"test.bin\") as rw:\n    for obj in rw.array_while_iterator(objs, MyStruct, MyStopCondition(0x10)):\n        rw.rw_obj(obj)\n        # Contrived example...\n        lst.append(obj.a)\n\n# [1, 2]\nprint(lst)\n</code></pre>"},{"location":"02-gettingstarted/#stream-alignment-and-offsets","title":"Stream Alignment and Offsets","text":"<p>Commonly, binary files contain offsets that point to the location of particular sections of data. Depending on the parse operation you are performing, you may want to do one of several things:</p> <ul> <li>Seek to the offset (for non-contiguous deserialization)</li> <li>Verify that your stream is currently at that offset (for contiguous deserialization and parse-like operations such as writing)</li> <li>Automatically calculate the value the offset should take</li> <li>Some user-defined operation</li> </ul> <p>Data sections in binary files are also often aligned to a cache-friendly width (for example, a data section may begin at a multiple of 0x10 bytes rather than at an arbitrary location).</p> <p><code>exbip</code> provides several utilities in the standard library for interacting with streams in this way, as described in the following sub-sections.</p>"},{"location":"02-gettingstarted/#seeks-and-tells","title":"Seeks and Tells","text":"<p>When needing to seek or align the stream, it is necessary to know where in the stream you currently are. Less commonly, you may also need to explicity seek within the file, such as if you need to write a chunk of deserialization-specific logic or define a custom operation.</p> <p>Multiple seek and tell operations are provided at a framework level. In all of these, 'seek' operations move the stream offset to a particular position, and 'tell' operations report a particular stream position.</p> <ul> <li><code>global_seek(offset)</code> and <code>global_tell(offset)</code> seek and tell relative to the beginning of the stream. These act like the <code>seek</code> and <code>tell</code> operations on the built-in Python stream objects (e.g. those returned by <code>open()</code> or <code>io.BytesIO()</code>)</li> <li><code>seek(offset)</code> and <code>tell(offset)</code> work slightly differently: these are context-sensitive seeks and tells that operate relative to the origin defined by the parser context. This is useful if, for example, you want to operate on a subfile that can be embedded within another file, but measures its offsets from the start of the subfile: by using context-sensitive seeks and tells, your code can work whether the struct is embedded or not, if you set a contextual origin. This is detailed in a later section.</li> <li><code>relative_global_seek(offset, base)</code> and <code>relative_global_tell(offset, base)</code> are context-free seeks and tells relative to the position <code>base</code>.</li> <li><code>relative_seek(offset, base)</code> and <code>relative_tell(offset, base)</code> are context-dependent seeks and tells relative to the position <code>base</code>.</li> </ul>"},{"location":"02-gettingstarted/#alignment","title":"Alignment","text":"<p>There are two stream alignment functions provided by the standard library:</p> <ul> <li><code>align(position, alignment[, pad_value=b'\\x00'])</code> will round the <code>position</code> up to the nearest multiple of <code>alignment</code>. If deserializing, it will read the bytes required to perform the alignment, and throw an exception if the read bytes are not equal to an array of the pad value. If the length of the read bytes is not divisible by the length of the pad value, an exception to this effect is thrown instead. When serializing, enough pad values to complete the alignment are written, and an exception is again thrown if the length of the pad value is not a factor of the length of alignment.</li> <li><code>fill(position, alignment[, fill_value=b'\\x00'])</code> behaves exactly as <code>align</code>, except it will not validate deserialized bytes against the <code>fill_value</code>.</li> </ul> <p>A typical usage might be:</p> <pre><code>class MyStruct:\n    def __init__(self, a, b, c):\n        self.a = a\n        self.b = b\n        self.c = c\n\n    def exbip_rw(self, rw):\n        self.a = rw.rw_uint32(self.a)  # Offset 0x00 -&gt; 0x04\n        self.b = rw.rw_uint16(self.b)  # Offset 0x04 -&gt; 0x06\n        self.c = rw.rw_float32(self.c) # Offset 0x06 -&gt; 0x0A\n        rw.align(rw.tell(), 0x10);     # Offset 0x0A -&gt; 0x10\n</code></pre>"},{"location":"02-gettingstarted/#offset-management","title":"Offset Management","text":"<p>For validation or navigation purposes, <code>exbip</code> provides a few way to move around the file depending on the parser you are using. The relevant functions for this section are:</p> <ul> <li><code>enforce_stream_offset(offset, message[, formatter=None, marker=None])</code></li> <li><code>navigate_stream_offset(offset, message[, formatter=None, marker=None])</code></li> <li><code>verify_stream_offset(offset, message[, formatter=None, marker=None])</code></li> <li><code>check_stream_offset(offset, message[, formatter=None, marker=None])</code></li> </ul> <p>In each of these, <code>offset</code> is a context-relative offset to be acted on, <code>message</code> is a debug message that will be included in any raised exceptions, <code>formatter</code> is an <code>exbip</code> formatter object that will safely format offsets without propagating <code>TypeError</code> or <code>ValueError</code> (e.g. caused by trying to format <code>None</code> to a hexstring), and <code>marker</code> is a special argument used for automatic offset calculation.</p> <p><code>enforce_</code> will seek to the provided <code>offset</code> in <code>deserialize</code> mode, raise an exception in <code>serialize</code> mode if <code>offset</code> is not at <code>rw.tell()</code>, and is ignored in <code>count</code> mode. This can be used for non-contiguous reads of a file.</p> <p><code>navigate_</code> will seek to the provided <code>offset</code> in <code>deserialize</code> mode, and is ignored in <code>serialize</code> and <code>count</code> mode. This can also be used for trickier non-contiguous reads of a file.</p> <p><code>verify_</code> will raise an exception in <code>deserialize</code> and <code>serialize</code> mode if <code>offset</code> is not at <code>rw.tell()</code>, and is ignored in <code>count</code> mode. This can be used for validating contiguous reads of a file.</p> <p><code>check_</code> is an alias that is set to <code>verify_</code> on all parsers except the <code>NonContiguousReader</code>. Generally this is the preferred function to use, since it gives you flexibility in whether you want to limit yourself to purely contiguous, strongly validated reads, or a flexible, but unvalidated, non-contiguous read. For most purposes, contiguous reads are fine since files are typically written in a strict structure, although a non-contiguous read is technically permitted by a format and may be the \"correct\" implementation.</p> <p>Tip</p> <p>When developing the serialization code for your structs, you may want to do this with a contiguous <code>Reader</code> class to take advantage of the offset validation. After the serialization routine is complete, you could then swap out the <code>Reader</code> in the <code>ReadableTrait</code> for a <code>NonContiguousReader</code>.</p>"},{"location":"02-gettingstarted/#automatic-offset-calculation","title":"Automatic Offset Calculation","text":"<p>The offsets of various significant locations within a file can be annoying to calculate, despite the fact that they are entirely predictable. <code>exbip</code> comes with a parser designed to perform a virtual write of the object, and during that write, it can report its current position as an offset to be stored in a variable.</p> <p>This is achieved with the <code>OffsetMarker</code> class. After constructing it, you can either: - Call <code>subscribe(obj, attr_name)</code> to add the <code>attr_name</code> attribute of <code>obj</code> to the list of variables to receive the offset reported by the marker, - Call <code>subscribe_callback(callback)</code> to provide a lambda that accepts the <code>OffsetCalculator</code> parser as its only argument, e.g. <code>marker.subscribe_callback(lambda rw: setattr(self, \"offset\", rw.tell() - 0x20))</code></p> <p>These <code>OffsetMarker</code>s can be invoked only by parsers that use the <code>calculate_offsets</code> descriptor method in two ways: - Calling <code>rw.dispatch_marker(marker)</code>, - Passing the marker as the fourth argument (<code>marker</code>) of <code>(enforce_/navigate_/verify_/check_)stream_offset</code>.</p> <p>Given below is an example of using the <code>OffsetMarker</code> to automatically calculate a list of offsets of some objects.</p> <pre><code>from exbip import Reader, Writer, OffsetCalculator, OffsetMarker\nfrom exbip import ReadableTrait, WriteableTrait, OffsetsCalculableTrait\nfrom exbip import HEX32_formatter\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n        self.offset = 0\n\n        # Used by OffsetCalculator\n        self.marker = OffsetMarker().subscribe(self, \"offset\")\n\n    def exbip_rw(self, rw):\n        self.offset = rw.rw_uint32(self.offset)\n\n    def rw_data(self, rw):\n        rw.verify_stream_offset(self.offset, \"Object offset\", HEX32_formatter, self.marker)\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyBiggerStruct(ReadableTrait(Reader), WriteableTrait(Writer), OffsetsCalculableTrait(OffsetCalculator)):\n    def __init__(self):\n        self.obj_count   = 0\n        self.objects     = []\n\n    def exbip_rw(self, rw):\n        self.obj_count   = rw.rw_uint32(self.obj_count)\n        self.objects = rw.rw_dynamic_objs(self.objects, MyStruct, self.obj_count)\n\n        for obj in self.objects:\n            obj.rw_data(rw)\n\n\nb = MyBiggerStruct()\nb.objects.append(MyStruct(1, 0.5))\nb.objects.append(MyStruct(2, 1.0))\nb.obj_count = len(b.objects)\n\n# Let the OffsetCalculator automatically calculate the offsets of each object\n\nprint(\"Before calculation:\", b.objects[0].offset, b.objects[1].offset)\nb.calculate_offsets()\nprint(\"After calculation:\", b.objects[0].offset, b.objects[1].offset)\nb.write(\"test.bin\")\n\nc = MyBiggerStruct()\nc.read(\"test.bin\")\nprint(\"Newly-read object:\", c.objects[0].offset, c.objects[1].offset)\n</code></pre>"},{"location":"02-gettingstarted/#binary-validation","title":"Binary Validation","text":"<p>You will commonly find that the data you need to serialize contains more data than the classes you want to use in your program. You may therefore want to employ specific \"serializable\" classes that represent the data structures in your files, and transform between your \"real\" classes when deserializing or serializing, or simply (de)serialize some data to/from placeholder variables. To detect bugs in this transformation process, the <code>Validator</code> can be extremely useful, and will pinpoint the exact data members that differ between two files/bytestrings.</p> <p>It is easiest to use the <code>Validator</code> with its corresponding trait, <code>ValidatableTrait</code>, since setting it up manually is a little bit involved. This gives you access to four functions:</p> <ul> <li><code>validate_file_against_file(primary_filepath, reference_filepath, *args, **kwargs)</code></li> <li><code>validate_file_against_bytes(primary_filepath, reference_bytes, *args, **kwargs)</code></li> <li><code>validate_bytes_against_file(primary_bytes, reference_filepath, *args, **kwargs)</code></li> <li><code>validate_bytes_against_bytes(primary_bytes, reference_bytes, *args, **kwargs)</code></li> </ul> <p>In each case, the required arguments are hopefully self-expanatory: you can use these functions to validate either a file or a bytestring against another file or bytestring, using an <code>exbip</code>-compatible object as the data structure for it. <code>*args</code> and <code>**kwargs</code> are passed to the <code>exbip_rw</code> function in all cases.</p> <p>Running one of these functions will throw a <code>ValidationError</code> if there is even a single byte that is different between your two deserialization sources. This is trivial to do in Python without the need for a class: you could simply <code>zip</code> two bytestrings together and check if all characters in the strings match. However, the <code>Validator</code> will throw the <code>ValidationError</code> from the exact read call that returns disagreeing bytes, giving you a full stacktrace through your deserialization procedure from the disagreeing data member up to the root validation call. This makes identifying exactly what has gone wrong significantly easier to diagnose.</p> <p>Let's take a look at an example to round out this section.</p> <pre><code>from exbip import Reader, Writer, OffsetCalculator, Validator,  OffsetMarker\nfrom exbip import ReadableTrait, WriteableTrait, OffsetsCalculableTrait, ValidatableTrait\nfrom exbip import HEX32_formatter\n\nclass MyStruct:\n    def __init__(self, a=None, b=None):\n        self.a = a\n        self.b = b\n\n        self.offset = 0\n\n        # Used by OffsetCalculator\n        self.marker = OffsetMarker().subscribe(self, \"offset\")\n\n    def exbip_rw(self, rw):\n        self.offset = rw.rw_uint32(self.offset)\n\n    def rw_data(self, rw):\n        rw.verify_stream_offset(self.offset, \"Object offset\", HEX32_formatter, self.marker)\n        self.a = rw.rw_uint32(self.a)\n        self.b = rw.rw_float32(self.b)\n\nclass MyBiggerStruct(ReadableTrait(Reader), WriteableTrait(Writer), OffsetsCalculableTrait(OffsetCalculator), ValidatableTrait(Validator)):\n    def __init__(self):\n        self.obj_count   = 0\n        self.objects     = []\n\n    def exbip_rw(self, rw):\n        self.obj_count   = rw.rw_uint32(self.obj_count)\n        self.objects = rw.rw_dynamic_objs(self.objects, MyStruct, self.obj_count)\n\n        for obj in self.objects:\n            obj.rw_data(rw)\n\n\nb = MyBiggerStruct()\nb.objects.append(MyStruct(1, 0.5))\nb.objects.append(MyStruct(2, 1.0))\nb.obj_count = len(b.objects)\n\n# Let the OffsetCalculator automatically calculate the offsets of each object\n\nb.calculate_offsets() # Oops, forgot to calculate the offsets!\nbstring1 = b.tobytes()\n# Contrived example, but it demonstrates the principle:\nb.objects[0].a = 10\nbstring2 = b.tobytes()\n\n# Read b'\\x01\\x00\\x00\\x00' from primary stream, reference data is b'\\n\\x00\\x00\\x00'\n# Provides a stacktrace stemming from \"self.a = rw.rw_uint32(self.a)\"\nMyBiggerStruct().validate_bytes_against_bytes(bstring1, bstring2)\n</code></pre>"},{"location":"02-gettingstarted/#serialization-context","title":"Serialization Context","text":""},{"location":"02-gettingstarted/#endianness","title":"Endianness","text":"<p>A data structure can be little-endian, big-endian, and sometimes it can contain both endiannesses. In addition to standard library functions to explicitly operate on data types of a particular endianness, it also supports functions that contextually use either little- or big-endianness at a framework level. Examples of these functions are <code>rw_uint32</code> and <code>rw_float32</code>.</p> <p>Whether these functions are in little- or big-endian mode depends on the parsing context; i.e. the state of the current parser. It is very easy to change this state using the framework-level context-manager functions <code>as_littleendian()</code>, <code>as_bigendian()</code>, and <code>as_endian(endianness)</code>. Below is an example of these functions in action.</p> <pre><code>from exbip import Writer\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    with rw.as_bigendian():\n        rw.rw_uint32(1) # Writes a big-endian int 1\n        with rw.as_littleendian():\n            rw.rw_uint32(2) # Writes a little-endian int 2\n        rw.rw_uint32(3) # Writes a big-endian int 3\n    rw.rw_uint32(4) # Default context: Writes a little-endian int 4\n    with rw.as_endian('little'): # Also '&lt;', '&gt;', or 'big'\n        rw.rw_uint32(5) # Writes a little-endian int 5\n</code></pre> <p>Note</p> <p>In order to mitigate performance loss, endian-aware functions are not implemented as wrapper functions or with runtime if/else statements. Instead, they are placeholder labels that the explicit little-endian and big-endian versions of an operator are monkey-patched onto. This monkey-patching occurs every time the endianness-context is switched. Therefore, if you need to extend a parser with your own functions that need to execute different functions for little-endian and big-endian invocations, you need to implement both versions and extend the parser with them.</p>"},{"location":"02-gettingstarted/#context-origin","title":"Context Origin","text":"<p>If you need to measure your <code>seek()</code>s and <code>tell()</code>s from somewhere other than the stream origin, you can set a context origin with a context manager object: <pre><code>from exbip import Writer\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_uint32s([0,1,2,3,4,5], 6) # Offset 0x00-&gt;0x18\n    print(hex(rw.tell())) # Prints 0x18\n    with rw.new_origin():\n        print(hex(rw.tell()), hex(rw.global_tell())) # Prints 0x0, 0x18\n    print(hex(rw.tell())) # Prints 0x18 again\n</code></pre> This can also be combined with the endianness context switches, e.g. <pre><code>from exbip import Writer\n\nwith Writer().FileIO(\"test.bin\") as rw:\n    rw.rw_uint32s([0,1,2,3,4,5], 6) # Offset 0x00-&gt;0x18\n    print(hex(rw.tell())) # Prints 0x18\n    with rw.new_origin(), rw.as_bigendian():\n        print(hex(rw.tell()), hex(rw.global_tell())) # Prints 0x0, 0x18\n    print(hex(rw.tell())) # Prints 0x18 again\n</code></pre></p>"},{"location":"02-gettingstarted/#dispatching-to-split-serialization-logic","title":"Dispatching to split serialization logic","text":"<p>Sometimes, the provided functionality is not sufficient and you need to write explicitly different code for your deserialization and serialization paths. To do this, you can capture your code in a 'descriptor' object:</p> <pre><code>class MyDescriptor:\n    def deserialize(binary_parser, ...):\n        ... # Here is the logic for deserialization\n\n    def serialize(binary_parser, ...):\n        ... # Here is the logic for serialization\n\n    def count(binary_parser, ...):\n        ... # Here is the logic for calculating how many bytes this invocation moves the stream offset\n</code></pre> <p>These three functions are the main ones used by the standard library: <code>Reader</code>-like parsers call into <code>deserialize</code>, <code>Writer</code>-like classes call into <code>serialize</code>, and <code>Counter</code>-like classes call into <code>count</code>. You can optionally define a <code>calculate_offsets</code> function that will be picked up by the <code>OffsetCalculator</code> instead of the <code>count</code> function.</p> <p>As example of this might be <pre><code>class MyDescriptor:\n    \"\"\"\n    A class that reads or writes one of two objects depending on the value of a bitvector.\n    \"\"\"\n    def deserialize(binary_parser, value, ctor1, ctor2, bitvector, count):\n        value.clear()\n\n        bv = bitvector\n        for _ in range(count):\n            if (bv &amp; 1):\n                value.push_back(ctor1())\n            else:\n                value.push_back(ctor2())\n            rw.rw_obj(value[-1])\n\n            bv &gt;&gt;= 1\n\n    def serialize(binary_parser, value, ctor1, ctor2, bitvector, count):\n        # You could do some validation here against the bitvector and/or count.\n        for obj in value:\n            rw.rw_obj(obj)\n\n    def count(binary_parser, value, ctor1, ctor2, bitvector, count):\n        for obj in value:\n            rw.rw_obj(obj)\n</code></pre></p> <p>The standard library provides the <code>rw_descriptor</code> function that can be used to dispatch to whichever of these functions is appropriate for the parser in use. To use it, simply pass in the descriptor along with any non-parser arguments it needs:</p> <pre><code>rw.rw_descriptor(MyDescriptor, arr, ctor1, ctor2, bitvector, count)\n</code></pre>"},{"location":"02-gettingstarted/#extending-with-new-operations","title":"Extending with new Operations","text":"<p>Once you know how to dispatch to split serialization logic, it is easy to create an operator extension for <code>exbip</code>. Instead of using your descriptor with <code>rw_descriptor</code>, we will simply promote its functionality to member functions of your parser classes.</p> <p>To do this, you just need to give it a class variable <code>FUNCTION_NAME</code> that will be the name of the member function, e.g.</p> <pre><code>class MyDescriptor:\n    FUNCTION_NAME = \"my_op\"\n\n    # deserialize, serialize, count, etc.\n</code></pre> <p>You can now extend any parser that uses the functions defined on your descriptor. To do this, you simply define a new parser that inherits from the object returned by the <code>extended_with</code> method of a parser. For example, you can make your own <code>Reader</code> and <code>Writer</code> like</p> <pre><code>class MyReader(Reader.extended_with([MyDescriptor])):\n    pass\n\nclass MyWriter(Writer.extended_with([MyDescriptor])):\n    pass\n</code></pre> <p>You will notice that <code>MyDescriptor</code> is in a list, allowing you to extend with multiple descriptors. In fact, it is wise to collect all your custom descriptors into a single list and extend your parsers with this list.</p> <pre><code>MyDescriptors = [MyDescriptor1, MyDescriptor2, MyDescriptor3]\n\nclass MyReader(Reader.extended_with(MyDescriptors)):\n    pass\n\nclass MyWriter(Writer.extended_with(MyDescriptors)):\n    pass\n</code></pre> <p>You can then use <code>my_op</code> whereever you use <code>MyReader</code> or <code>MyWriter</code>:</p> <pre><code>with MyWriter().FileIO(\"test.bin\") as rw:\n    rw.my_op(...)\n</code></pre> <p>You can also use <code>MyReader</code> and <code>MyWriter</code> as the parsers for traits, which can be packaged up into a convenient base class for your structs:</p> <pre><code>class MySerializable(ReadableTrait(MyReader),\n                     WriteableTrait(MyWriter)):\n    pass\n\nclass MyStruct1(MySerializable):\n    def __init__(self):\n        self.a = 10\n\n    def exbip_rw(self, rw):\n        rw.my_op(...)\n</code></pre> <p>You can create sub-libraries for <code>exbip</code> by providing these lists of descriptors for custom data types that others can use to extend <code>exbip</code> with.</p>"},{"location":"02-gettingstarted/#extending-with-new-endian-aware-operators","title":"Extending with new endian-aware operators","text":"<p>The <code>extended_with</code> function described in the previous section can take a second argument: a list of endian-aware functions. This is a little bit more complicated to set up and is aided by a framework class from <code>exbip</code>.</p> <p>To start with, you need to define your little-endian and your big-endian descriptors:</p> <pre><code>class MyLEDescriptor:\n    FUNCTION_NAME = \"my_op_le\"\n\n    # deserialize, serialize, count\n\nclass MyBEDescriptor:\n    FUNCTION_NAME = \"my_op_be\"\n\n    # deserialize, serialize, count\n</code></pre> <p>From here we can define an <code>EndianPairDescriptor</code>:</p> <pre><code>from exbip.framework import EndianPairDescriptor\n\nMyDescriptor = EndianPairDescriptor(\"my_op\", \"my_op_le\", \"my_op_be\")\n</code></pre> <p>where the first argument is the endian-aware function name, the second argument is the name of the little-endian function, and the third argument is the big-endian function name. You could of course alternatively write this as</p> <pre><code>from exbip.framework import EndianPairDescriptor\n\nMyDescriptor = EndianPairDescriptor(\"my_op\", MyLEDescriptor.FUNCTION_NAME, MyBEDescriptor.FUNCTION_NAME)\n</code></pre> <p>You can then install your endian-aware function in the second argument of the <code>extended_with</code> function:</p> <pre><code>class MyReader([MyLEDescriptor, MyBEDescriptor], [MyDescriptor]):\n    pass\n</code></pre>"},{"location":"02-gettingstarted/#extending-with-new-parsers","title":"Extending with new parsers","text":"<p>You can define entirely new parser types as long as they conform to the <code>exbip</code> interface. In principle, you only need a few things: a member variable called <code>_bytestream</code>, a static <code>_get_rw_method</code> method that retreives the appropriate operator implementation from a descriptor, an implementation for <code>global_seek</code> and <code>global_tell</code>, and to inherit from <code>IBinaryParser</code>.</p> <p>As an example, here is a somewhat arbitrarily-defined custom parser.</p> <pre><code>class MyCustomParserBase(IBinaryParser):\n    def __init__(self):\n        super().__init__()\n        self._bytestream = None\n\n    @classmethod\n    def new(cls, initializer):\n        instance = cls()\n        instance._bytestream = io.BytesIO(initializer)\n        return instance\n\n    def global_tell(self):\n        return self._bytestream.tell()\n\n    def global_seek(self, offset):\n        return self._bytestream.seek(offset)\n\n    @staticmethod\n    def _get_rw_method(descriptor):\n        if hasattr(descriptor, \"custom_op\"):\n            return descriptor.custom_op\n        else:\n            return descriptor.count\n</code></pre> <p>You can install the standard library descriptors onto this like </p> <pre><code>from exbip import STANDARD_DESCRIPTORS, STANDARD_ENDIAN_DESCRIPTORS\nclass MyCustomParser(MyCustomParserBase.extended_with(STANDARD_DESCRIPTORS, STANDARD_ENDIAN_DESCRIPTORS)):\n    pass\n</code></pre> <p>and use it like</p> <pre><code># No reason why we also couldn't have written the constructor\n# to take an initializer, or to set the _bytestream to something\n# directly. The programmer should design their parser to solve\n# the particular problem they need it to solve.\nwith MyCustomParser.new(b'') as rw:\n    ... # operations\n</code></pre> <p>If appropriate, you could also write yourself a trait for this class that can be used as a mix-in for any serializables you write:</p> <pre><code>def CustomParseableTrait(CustomParser):\n    class CustomParseableTraitImpl:\n        def custom_parse(self, *args, **kwargs):\n            custom_parser = CustomParser()\n            custom_parser.rw_obj(self, *args, **kwargs)\n\n    return CustomParseableTraitImpl\n</code></pre> <p>To summarise, this is a parser that wraps an <code>io.BytesIO</code> stream and will call the <code>custom_op</code> function of a descriptor if it exists, and otherwise fall back to the <code>count</code> operator. You could make it depend only on the <code>custom_op</code> function existing -- however, your parser would then not be compatible with the standard library descriptors since this function does not exist on any of these. You would either need to define new descriptors inheriting from the standard library that implement this operation and extend your base class with these, or write a completely custom set of descriptors that conform to a different interface than that implemented by the standard library descriptors.</p>"},{"location":"02-gettingstarted/#whats-next","title":"What's next?","text":"<p>This article should have covered the main features of <code>exbip</code>. A listing of all standard descriptors and framework functionality is provided in the Standard Library and Framework pages, which this article as hopefully prepared you to understand.</p>"},{"location":"03-standardlibrary/","title":"Standard Library","text":"<p>Namespace: <code>exbip</code></p>"},{"location":"03-standardlibrary/#parsers","title":"Parsers","text":"<p> <code> Reader #  Inherits from <code>ReaderBase</code>. Implements the standard library descriptors. A parser for deserializing bytes to Python objects. Designed for contiguous parsing.  <p> Descriptor Method: <code>deserialize</code> Operator Aliases:   check_stream_offset -&gt; verify_stream_offset</p> <p> <code> NonContiguousReader #  Inherits from <code>ReaderBase</code>. Implements the standard library descriptors. A parser for deserializing bytes to Python objects. Designed for non-contiguous parsing.  <p> Descriptor Method: <code>deserialize</code> Operator Aliases:   check_stream_offset -&gt; enforce_stream_offset</p> <p> <code> Validator #  Inherits from <code>ValidatorBase</code>. Implements the standard library descriptors. A parser to verify whether two bytestreams deserialize to identical Python objects, and if not, to raise an error that originates at the first pair of read calls that deserialize to different values. Designed for contiguous parsing.  <p> Descriptor Method: <code>deserialize</code> Operator Aliases:   check_stream_offset -&gt; verify_stream_offset</p> <p> <code> Writer #  Inherits from <code>WriterBase</code>. Implements the standard library descriptors. A parser for serializing Python objects to bytes. Designed for contiguous parsing.  <p> Descriptor Method: <code>serialize</code> Operator Aliases:   check_stream_offset -&gt; verify_stream_offset</p> <p> <code> Counter #  Inherits from <code>CounterBase</code>. Implements the standard library descriptors. A parser for counting how many bytes a stream pointer will advance for each operator call. Designed for contiguous parsing.  <p> Descriptor Method: <code>count</code> Operator Aliases:   check_stream_offset -&gt; verify_stream_offset</p> <p> <code> OffsetCalculator #  Inherits from <code>OffsetCalculatorBase</code>. Implements the standard library descriptors. A parser for counting how many bytes a stream pointer will advance for each operator call, and using this to calculate the value of stream-offset variables within a structure.  <p> Descriptor Method: <code>calculate_offsets</code> if it exists. Else, <code>count</code> Operator Aliases:   check_stream_offset -&gt; verify_stream_offset</p>"},{"location":"03-standardlibrary/#traits","title":"Traits","text":"<p> <code> ReadableTrait(Reader): #  A trait that adds deserialization methods to a structure.   Functions:   read(filepath, \\*args, \\*\\*kwargs):   Passes <code>filepath</code> to the <code>FileIO()</code> method of <code>Reader</code>.   frombytes(byte_data, \\*args, \\*\\*kwargs):   Passes <code>byte_data</code> to the <code>BytestreamIO()</code> method of <code>Reader</code>.  <p> <code> WriteableTrait(Writer): #  A trait that adds serialization methods to a structure.   Functions:   write(filepath, \\*args, \\*\\*kwargs):   Passes <code>filepath</code> to the <code>FileIO()</code> method of <code>Writer</code>.   tobytes(\\*args, \\*\\*kwargs):   Returns the bytes written to the stream initialized by the <code>BytestreamIO()</code> method of <code>Writer</code>.  <p> <code> ValidatableTrait(Validator): #  A trait that adds validation methods to a structure.   Functions:   validate_file_against_file(primary_filepath, reference_filepath, \\*args, \\*\\*kwargs):   Validates the data in <code>primary_filepath</code> against the data in <code>reference_filepath</code>.   validate_file_against_bytes(primary_filepath, reference_bytes,    \\*args, \\*\\*kwargs):   Validates the data in <code>primary_filepath</code> against the bytes in <code>reference_bytes</code>.   validate_bytes_against_file(primary_bytes,    reference_filepath, \\*args, \\*\\*kwargs):   Validates the bytes in <code>primary_bytes</code> against the data in <code>reference_filepath</code>.   validate_bytes_against_bytes(primary_bytes,    reference_bytes,    \\*args, \\*\\*kwargs):   Validates the bytes in <code>primary_bytes</code> against the bytes in <code>reference_bytes</code>.  <p> <code> OffsetsCalculableTrait(OffsetCalculator): #  A trait that adds offset calculation methods to a structure.   Functions:   calculate_offsets(obj, \\*args, \\*\\*kwargs):   Calls <code>OffsetCalculator.rw_obj(obj, \\*args, \\*\\*kwargs)</code> on an instance of <code>OffsetCalculator</code>. This is intended to be used to automatically calculate the values of file offsets."},{"location":"03-standardlibrary/#operations","title":"Operations","text":""},{"location":"03-standardlibrary/#primitives","title":"Primitives","text":"<p> <code> rw_int8(value): #  Operates on a signed 8-bit integer with context-sensitive endianness.   deserialize:   Returns a signed 8-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 1 byte and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 1. Returns <code>value</code>.   Variants:  <code>rw_int8_le(value)</code>  Forced little-endian mode.  <code>rw_int8_be(value)</code>  Forced big-endian mode.  <code>rw_int8_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int16(value): #  Operates on a signed 16-bit integer with context-sensitive endianness.   deserialize:   Returns a signed 16-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 2 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 2. Returns <code>value</code>.   Variants:  <code>rw_int16_le(value)</code>  Forced little-endian mode.  <code>rw_int16_be(value)</code>  Forced big-endian mode.  <code>rw_int16_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int32(value): #  Operates on a signed 32-bit integer with context-sensitive endianness.   deserialize:   Returns a signed 32-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 4 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 4. Returns <code>value</code>.   Variants:  <code>rw_int32_le(value)</code>  Forced little-endian mode.  <code>rw_int32_be(value)</code>  Forced big-endian mode.  <code>rw_int32_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int64(value): #  Operates on a signed 64-bit integer with context-sensitive endianness.   deserialize:   Returns a signed 64-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 8 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 8. Returns <code>value</code>.   Variants:  <code>rw_int64_le(value)</code>  Forced little-endian mode.  <code>rw_int64_be(value)</code>  Forced big-endian mode.  <code>rw_int64_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint8(value): #  Operates on an unsigned 8-bit integer with context-sensitive endianness.   deserialize:   Returns an unsigned 8-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 1 byte and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 1. Returns <code>value</code>.   Variants:  <code>rw_uint8_le(value)</code>  Forced little-endian mode.  <code>rw_uint8_be(value)</code>  Forced big-endian mode.  <code>rw_uint8_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint16(value): #  Operates on an unsigned 16-bit integer with context-sensitive endianness.   deserialize:   Returns an unsigned 16-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 2 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 2. Returns <code>value</code>.   Variants:  <code>rw_uint16_le(value)</code>  Forced little-endian mode.  <code>rw_uint16_be(value)</code>  Forced big-endian mode.  <code>rw_uint16_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint32(value): #  Operates on an unsigned 32-bit integer with context-sensitive endianness.   deserialize:   Returns an unsigned 32-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 4 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 4. Returns <code>value</code>.   Variants:  <code>rw_uint32_le(value)</code>  Forced little-endian mode.  <code>rw_uint32_be(value)</code>  Forced big-endian mode.  <code>rw_uint32_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint64(value): #  Operates on an unsigned 64-bit integer with context-sensitive endianness.   deserialize:   Returns an unsigned 64-bit integer unpacked from the stream.   serialize:   Packs <code>value</code> to 8 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 8. Returns <code>value</code>.   Variants:  <code>rw_uint64_le(value)</code>  Forced little-endian mode.  <code>rw_uint64_be(value)</code>  Forced big-endian mode.  <code>rw_uint64_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float16(value): #  Operates on an IEEE 16-bit float with context-sensitive endianness.   deserialize:   Returns an IEEE 16-bit float unpacked from the stream.   serialize:   Packs <code>value</code> to 2 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 2. Returns <code>value</code>.   Variants:  <code>rw_float16_le(value)</code>  Forced little-endian mode.  <code>rw_float16_be(value)</code>  Forced big-endian mode.  <code>rw_float16_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float32(value): #  Operates on an IEEE 32-bit float with context-sensitive endianness.   deserialize:   Returns an IEEE 32-bit float unpacked from the stream.   serialize:   Packs <code>value</code> to 4 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 4. Returns <code>value</code>.   Variants:  <code>rw_float32_le(value)</code>  Forced little-endian mode.  <code>rw_float32_be(value)</code>  Forced big-endian mode.  <code>rw_float32_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float64(value): #  Operates on an IEEE 64-bit float with context-sensitive endianness.   deserialize:   Returns an IEEE 64-bit float unpacked from the stream.   serialize:   Packs <code>value</code> to 8 bytes and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by 8. Returns <code>value</code>.   Variants:  <code>rw_float64_le(value)</code>  Forced little-endian mode.  <code>rw_float64_be(value)</code>  Forced big-endian mode.  <code>rw_float64_e(value, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively."},{"location":"03-standardlibrary/#primitive-arrays","title":"Primitive Arrays","text":"<p> <code> rw_int8s(value, shape): #  Operates on an array of signed 8-bit integers with context-sensitive endianness.   deserialize:   Returns an array of signed 8-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_int8s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_int8s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_int8s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int16s(value, shape): #  Operates on an array of signed 16-bit integers with context-sensitive endianness.   deserialize:   Returns an array of signed 16-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_int16s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_int16s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_int16s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int32s(value, shape): #  Operates on an array of signed 32-bit integers with context-sensitive endianness.   deserialize:   Returns an array of signed 32-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_int32s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_int32s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_int32s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_int64s(value, shape): #  Operates on an array of signed 64-bit integers with context-sensitive endianness.   deserialize:   Returns an array of signed 64-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_int64s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_int64s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_int64s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint8s(value, shape): #  Operates on an array of unsigned 8-bit integers with context-sensitive endianness.   deserialize:   Returns an array of unsigned 8-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_uint8s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_uint8s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_uint8s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint16s(value, shape): #  Operates on an array of unsigned 16-bit integers with context-sensitive endianness.   deserialize:   Returns an array of unsigned 16-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_uint16s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_uint16s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_uint16s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint32s(value, shape): #  Operates on an array of unsigned 32-bit integers with context-sensitive endianness.   deserialize:   Returns an array of unsigned 32-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_uint32s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_uint32s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_uint32s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_uint64s(value, shape): #  Operates on an array of unsigned 64-bit integers with context-sensitive endianness.   deserialize:   Returns an array of unsigned 64-bit integers unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_uint64s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_uint64s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_uint64s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float16s(value, shape): #  Operates on an array of IEEE 16-bit floats with context-sensitive endianness.   deserialize:   Returns an array of IEEE 16-bit floats unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_float16s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_float16s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_float16s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float32s(value, shape): #  Operates on an array of IEEE 32-bit floats with context-sensitive endianness.   deserialize:   Returns an array of IEEE 32-bit floats unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_float32s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_float32s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_float32s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively.  <p> <code> rw_float64s(value, shape): #  Operates on an array of IEEE 64-bit floats with context-sensitive endianness.   deserialize:   Returns an array of IEEE 64-bit floats unpacked from the stream, row-major reshaped to the dimensions given by <code>shape</code>.   serialize:   Packs <code>value</code> in row-major order and writes it to the stream. Returns <code>value</code>.   count:   Increments the stream offset by the binary size of <code>value</code>. Returns <code>value</code>.   Variants:  <code>rw_float64s_le(value, shape)</code>  Forced little-endian mode.  <code>rw_float64s_be(value, shape)</code>  Forced big-endian mode.  <code>rw_float64s_e(value, shape, endianness)</code>  Runtime endianness. Pass <code>'&lt;'</code> or <code>'&gt;'</code> for little- or big-endianness respectively."},{"location":"03-standardlibrary/#strings","title":"Strings","text":"<p> <code> rw_bytestring(value, length): #  Operates on a fixed-size bytestring.   deserialize:   Reads <code>length</code> chars from the stream. Returns a <code>bytes</code> object.   serialize:   Writes <code>value</code> to the stream. Expects <code>value</code> to be a <code>bytes</code>-like object. Returns <code>value</code>.   count:   Increments the stream offset by the length of <code>value</code>. Expects <code>value</code> to be a <code>bytes</code> object. Returns <code>value</code>.  <p> <code> rw_bytestrings(value, lengths): #  Operates on a list of fixed-size bytestrings.   deserialize:   Reads <code>length</code> chars from the stream. Returns a <code>List[bytes]</code> object.   serialize:   Writes each element in <code>value</code> to the stream. Expects <code>value</code> to be an iterable of <code>bytes</code>-like objects. Returns <code>value</code>.   count:   Increments the stream offset by the sum of all lengths in <code>value</code>. Expects <code>value</code> to be an iterable of <code>bytes</code>-like objects. Returns <code>value</code>.  <p> <code> rw_cbytestring(value[, chunksize=0x40, terminator=b'\\x00']): #  Operates on a terminated bytestring.   deserialize:   Reads chars from the stream in blocks of <code>chunksize</code> until the <code>terminator</code> pattern is identified in one of those blocks. Returns a <code>bytes</code> object without the terminator.   serialize:   Writes <code>value</code> to the stream, and then writes the <code>terminator</code>. Expects <code>value</code> to be a <code>bytes</code>-like object. Returns <code>value</code>.   count:   Increments the stream offset by the length of <code>value</code> plus the length of the <code>terminator</code>. Expects <code>value</code> to be an iterable of <code>bytes</code>-like objects. Returns <code>value</code>.  <p> <code> rw_cbytestrings(value, count[, chunksize=0x40, terminator=b'\\x00']): #  Operates on a list of terminated bytestrings.   deserialize:   Reads chars from the stream in blocks of <code>chunksize</code> until the <code>terminator</code> pattern is identified in one of those blocks, <code>count</code> times. Returns a <code>List[bytes]</code> object.   serialize:   Writes each element (followed by the <code>terminator</code>) in <code>value</code> to the stream. Expects <code>value</code> to be an iterable of <code>bytes</code>-like objects. Returns <code>value</code>.   count:   Increments the stream offset by the sum of all lengths in <code>value</code> (plus the requisite <code>terminator</code> lengths). Expects <code>value</code> to be an iterable of <code>bytes</code>-like objects. Returns <code>value</code>."},{"location":"03-standardlibrary/#descriptors","title":"Descriptors","text":"<p> <code> rw_descriptor(descriptor, *args, **kwargs): #  Looks for and executes the descriptor method required by the parser.   deserialize:   Invokes the 'deserialize' method of the descriptor.   serialize:   Invokes the 'serialize' method of the descriptor.   count:   Invokes the 'count' method of the descriptor."},{"location":"03-standardlibrary/#objects","title":"Objects","text":"<p> <code> rw_obj(value, *args, **kwargs): #  Executes the <code>exbip_rw</code> function on <code>value</code>.   deserialize:   Calls <code>exbip_rw</code> on <code>value</code>.   serialize:   Calls <code>exbip_rw</code> on <code>value</code>.   count:   Calls <code>exbip_rw</code> on <code>value</code>.  <p> <code> rw_dynamic_obj(value, constructor, *args, **kwargs): #  Executes the <code>exbip_rw</code> function on <code>value</code> if not deserializing, otherwise constructs it first.   deserialize:   Calls <code>constructor</code> to build a new object, then calls <code>exbip_rw</code> on it.   serialize:   Calls <code>exbip_rw</code> on <code>value</code>.   count:   Calls <code>exbip_rw</code> on <code>value</code>.  <p> <code> rw_dynamic_objs(values, constructor, count, *args, **kwargs): #  Executes the <code>exbip_rw</code> function on <code>value</code> if not deserializing, otherwise constructs to object array first from a count.   deserialize:   Calls <code>constructor</code> to build a new object, then calls <code>exbip_rw</code> on it, <code>count</code> times.   serialize:   Calls <code>exbip_rw</code> on each element in <code>values</code>.   count:   Calls <code>exbip_rw</code> on each element in <code>values</code>.  <p> <code> rw_dynamic_objs_while(values, constructor, condition, *args, **kwargs): #  Executes the <code>exbip_rw</code> function on <code>value</code> if not deserializing, otherwise constructs to object array first from a boolean condition.   deserialize:   Calls <code>constructor</code> to build a new object, then calls <code>exbip_rw</code> on it, until calling <code>condition</code> returns False. <code>condition</code> must be a function (or functor) that takes the parser as its only argument.   serialize:   Calls <code>exbip_rw</code> on each element in <code>values</code>.   count:   Calls <code>exbip_rw</code> on each element in <code>values</code>."},{"location":"03-standardlibrary/#iterators","title":"Iterators","text":"<p> <code> array_iterator(array, constructor, count): #  Returns an iterable object that will construct <code>count</code> array elements if deserializing, and iterate through an array if not. Example:  <pre><code>for i, obj in enumerate(rw.array_iterator(arr, MyObject, my_count)):\n    rw.check_stream_offset(obj_offsets[i], \"Object {i}\", HEX32_FORMATTER)\n    rw.rw_obj(obj)\n</code></pre> This can be used for more advanced looping behavior than e.g. <code>rw_dynamic_objs</code>.   deserialize:   Returns an iterable object that clears the given array, and then appends a newly-constructed object to the array at the start of each iteration.   serialize:   Returns an iterable object that iterates through the elements of <code>array</code>.   count:   Returns an iterable object that iterates through the elements of <code>array</code>.  <p> <code> array_while_iterator(array, constructor, stop_condition): #  Returns an iterable object that will construct array elements until <code>condition</code> is False if deserializing, and iterate through an array if not. Example:  <pre><code>class Cond:\n    def __init__(self, offset):\n        self.offset = offset\n\n    def __call__(self, rw):\n        return rw.tell() &lt; self.offset\n\noffset_list = []\nfor obj in rw.array_while_iterator(arr, MyObject, Cond(offset)):\n    offset_list.append(rw.tell())\n    rw.rw_obj(obj)\n</code></pre> This can be used for more advanced looping behavior than e.g. <code>rw_dynamic_objs_while</code>.   deserialize:   Returns an iterable object that clears the given array, and then appends a newly-constructed object to the array at the start of each iteration.   serialize:   Returns an iterable object that iterates through the elements of <code>array</code>.   count:   Returns an iterable object that iterates through the elements of <code>array</code>."},{"location":"03-standardlibrary/#sections","title":"Sections","text":"<p> <code> section_exists(offset, count): #  Returns a bool that states whether a data section defined by a non-zero offset and/or count exists or not.   deserialize:   Returns True if <code>offset</code> is greater than 0.   serialize:   Returns True if <code>count</code> is greater than 0.   count:   Returns True if <code>count</code> is greater than 0."},{"location":"03-standardlibrary/#stream-end-of-file","title":"Stream End-of-file","text":"<p> <code> assert_eof(): #  Raises an exception if the stream pointer is not at the end of the stream, if the parsing mode defines an end-of-stream.   deserialize:   Raises a <code>NotAtEOFError</code> if the stream pointer is not at the end of the stream.   serialize:   Does nothing.   count:   Does nothing."},{"location":"03-standardlibrary/#stream-alignment","title":"Stream Alignment","text":"<p> <code> align(offset, alignment(, pad_value=b'\\x00'): #  Rounds up <code>offset</code> to the next multiple of <code>alignment</code>, and performs an operation on the section of the stream corresponding to the skipped bytes.   deserialize:   Reads enough bytes to perform the alignment, and validates that they are an array of <code>pad_value</code>s. Raises <code>ValueError</code> if the number of bytes required for the alignment is not divisible by the length of <code>pad_value</code>.   serialize:   Writes enough <code>pad_value</code>s to the stream to perform the alignment. Raises <code>ValueError</code> if the number of bytes required for the alignment is not divisible by the length of <code>pad_value</code>.   count:   Advances the stream offset to the next multiple of <code>alignment</code>. Raises <code>ValueError</code> if the number of bytes required for the alignment is not divisible by the length of <code>pad_value</code>.  <p> <code> fill(offset, alignment(, fill_value=b'\\x00'): #  Rounds up <code>offset</code> to the next multiple of <code>alignment</code>, and performs an operation on the section of the stream corresponding to the skipped bytes.   deserialize:   Reads enough bytes to perform the alignment. Does not perform any validation.   serialize:   Writes enough <code>pad_value</code>s to the stream to perform the alignment. Raises <code>ValueError</code> if the number of bytes required for the alignment is not divisible by the length of <code>pad_value</code>.   count:   Advances the stream offset to the next multiple of <code>alignment</code>. Raises <code>ValueError</code> if the number of bytes required for the alignment is not divisible by the length of <code>pad_value</code>."},{"location":"03-standardlibrary/#stream-offsets","title":"Stream Offsets","text":"<p> <code> verify_stream_offset(offset, message[, formatter=lambda x: x, notifier=None]): #  Seeks to <code>offset</code>, throws an error if the stream offset is not at <code>offset</code>, or calls <code>notifier</code> with the current stream offset.   deserialize:   Seeks to <code>offset</code>.   serialize:   Throws a <code>UnexpectedOffsetError</code> if the stream offset is not at <code>offset</code>.   count:   Does nothing.   calculate_offsets:   Calls <code>notifier.notify()</code> if <code>notifier</code> is not None. Commonly, <code>notifier</code> is an <code>OffsetMarker</code> object.  <p> <code> navigate_stream_offset(offset, message[, formatter=lambda x: x, notifier=None]): #  Seeks to <code>offset</code>, throws an error if the stream offset is not at <code>offset</code>, or calls <code>notifier</code> with the current stream offset.   deserialize:   Seeks to <code>offset</code>.   serialize:   Does nothing.   count:   Does nothing.   calculate_offsets:   Calls <code>notifier.notify()</code> if <code>notifier</code> is not None. Commonly, <code>notifier</code> is an <code>OffsetMarker</code> object.  <p> <code> verify_stream_offset(offset, message[, formatter=lambda x: x, notifier=None]): #  Throws an error if the stream offset is not at <code>offset</code>, or calls <code>notifier</code> with the current stream offset.   deserialize:   Throws a <code>UnexpectedOffsetError</code> if the stream offset is not at <code>offset</code>.   serialize:   Throws a <code>UnexpectedOffsetError</code> if the stream offset is not at <code>offset</code>.   count:   Does nothing.   calculate_offsets:   Calls <code>notifier.notify()</code> if <code>notifier</code> is not None. Commonly, <code>notifier</code> is an <code>OffsetMarker</code> object.  <p> <code> check_stream_offset(offset, message[, formatter=lambda x: x, notifier=None]): #  An alias for <code>verify_stream_offset</code>. Used by all standard parsers except <code>NonContiguousReader</code>.  <p> <code> check_stream_offset(offset, message[, formatter=lambda x: x, notifier=None]): #  An alias for <code>enforce_stream_offset</code>. Used by <code>NonContiguousReader</code>."},{"location":"03-standardlibrary/#stream-padding","title":"Stream Padding","text":"<p> <code> rw_padding(count[, pad_value=b'\\x00', validate=True]): #  Operates on an anonymous bytestring of length <code>count</code>.   deserialize:   Reads <code>count*len(pad_value)</code> bytes from the stream. If <code>validate</code> is True, the bytes are checked to ensure they are an array of <code>pad_value</code>. If they are not, an <code>UnexpectedPaddingError</code> is raised.   serialize:   Writes <code>count</code> <code>pad_value</code>s to the stream.   count:   Advances the stream by <code>count*len(pad_value)</code>."},{"location":"03-standardlibrary/#offset-markers","title":"Offset Markers","text":"<p> <code> dispatch_marker(marker): #  Operates on an object with a single-argument <code>notify</code> method, expected to be an <code>OffsetMarker</code> object.   deserialize:   Does nothing.   serialize:   Does nothing.   count:   Does nothing.   calculate_offsets:   Calls <code>marker.notify()</code>."},{"location":"03-standardlibrary/#formatters","title":"Formatters","text":"<p> <code> safe_formatter(formatter): #  Returns a <code>safe_formatter</code> object that calls <code>formatter</code>, which is a format method like e.g. <code>'0b{0:0&gt;8b}'.format</code>, and returns the call return value if it does not throw an exception. On throwing a <code>ValueError</code> or <code>TypeError</code>, the original value is returned. Other exceptions are uncaught.  <p> <code> bin8_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as an 8-bit binary string.  <p> <code> bin16_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 16-bit binary string.  <p> <code> bin32_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 32-bit binary string.  <p> <code> bin64_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 64-bit binary string.  <p> <code> hex8_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as an 8-bit lower-case hex string.  <p> <code> hex16_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 16-bit lower-case hex string.  <p> <code> hex32_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 32-bit lower-case hex string.  <p> <code> hex64_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 64-bit lower-case hex string.  <p> <code> HEX8_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as an 8-bit upper-case hex string.  <p> <code> HEX16_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 16-bit upper-case hex string.  <p> <code> HEX32_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 32-bit upper-case hex string.  <p> <code> HEX64_formatter(value): #  A <code>safe_formatter</code> that formats <code>value</code> as a 64-bit upper-case hex string."},{"location":"03-standardlibrary/#structures","title":"Structures","text":"<p><code> OffsetMarker #  A class that holds a list of single-argument callbacks. When <code>notify</code> is called, the offset passed to <code>notify</code> is used to call each of these callbacks, each of which is intended to give a particular variable the value of that offset.   Functions:   clear():   Clears the subscriber list.   subscribe(obj, attr):   Adds a callback <code>lambda offset: setattr(obj, attr, offset)</code> to the <code>OffsetMarker</code>'s callbacks list. Returns <code>self</code> so that multiple subscriptions can be chained with the constructor.   subscribe_callback(callback):   Adds the single-argument function <code>callback</code> to the <code>OffsetMarker</code>'s callbacks list. Returns <code>self</code> so that multiple subscriptions can be chained with the constructor.   notify(offset):   Iterates through the list of subscribers and calls each one with <code>offset</code>."},{"location":"04-framework/","title":"Framework","text":"<p>Namespace: <code>exbip.framework</code></p>"},{"location":"04-framework/#descriptors","title":"Descriptors","text":"<p><code> EndianPairDescriptor #  Instantiate this class to create an endian-aware descriptor.   Functions:   __init__(name, little_endian, big_endian):   When installed on a parser, this will create an operation called <code>name</code> that is set to the operation with name <code>little_endian</code> in little-endian mode and with name <code>big_endian</code> when in big-endian mode."},{"location":"04-framework/#parsers","title":"Parsers","text":"<p><code> IBinaryParser #  A base class for <code>exbip</code> parsers.  <p> Descriptor Method: undefined Additional Functions: </p>  _get_rw_method(descriptor):   Pure virtual method. Must be implemented by an inheriting class. Intended to return a function on <code>descriptor</code> matching the theme of the parser.   global_seek(position):   Pure virtual method. Must be implemented by an inheriting class. Intended to set the stream offset to the given offset.   global_tell():   Pure virtual method. Must be implemented by an inheriting class. Intended to return the stream offset.   [classmethod] extended_with(descriptors, endian_inlined):   Returns a class derived from the current class that has the operators defined in <code>descriptors</code> defined on it, using the <code>rw_method</code> for that class. Also installs the endian-aware descriptors in <code>endian_inlined</code>.   execute_descriptor(descriptor, \\*args, \\*\\*kwargs):   Fetches the required method from <code>descriptor</code> for this parser and executes it.   __call__(descriptor, \\*args, \\*\\*kwargs):   Equal to <code>execute_descriptor</code>.   [staticmethod] bytes_to_alignment(position, alignment):   Calculates how many bytes are required to round <code>position</code> up to the nearest multiple of <code>alignmente</code>.   is_unaligned(alignment):   Returns True if the current stream position is not a multiple of <code>alignment</code>, else, False.   relative_global_seek(offset, base_position):   Seeks to <code>offset + base_position</code> relative to the stream origin.   seek(offset):   Seeks to <code>offset</code> relative to the context origin.   relative_seek(offset, base_position):   Seeks to <code>offset + base_position</code> relative to the context origin.   relative_global_tell(base_position):   Returns <code>global_tell() - base_position</code>.   tell():   Returns <code>global_tell()</code> relative to the context origin.   relative_tell(base_position):   Returns <code>global_tell() - base_position</code> relative to the context origin.   local_to_global_offset(offset):   Transforms <code>offset</code> from a local (context) offset to a global (stream) offset.   global_to_local_offset(offset):   Transforms <code>offset</code> from a global (stream) offset to a local (context) offset.   current_origin():   Returns the current stream position corresponding to the context origin.   push_origin(offset):   Sets <code>offset</code> as the current context origin.   pop_origin():   Restores the context origin to the previous context's origin.   new_origin():   Returns a context manager that will call <code>push_origin</code> when entered, and <code>pop_origin</code> when exited.   [property] endianness():   Returns the current context's endianness.   set_endianness(endianness):   Sets the current context's endianness. Valid values are <code>'&lt;'</code> and <code>'&gt;'</code>.   as_littleendian():   Returns a context manager that sets the context endianness to little endian for the scope of the context.   as_bigendian():   Returns a context manager that sets the context endianness to big endian for the scope of the context.   as_endian(endianness):   Returns a context manager that sets the context endianness to the provided endianness for the scope of the context. Valid values are <code>'&lt;'</code>, <code>'&gt;'</code>, <code>'little'</code>, and <code>'big'</code>.   assert_equal(input_value, reference_value[, value_name=None, formatter=None]):   Throws an exception if <code>input_value</code> does not equal <code>reference_value</code>. The <code>name</code> of the value is included in the exception if it is not <code>None</code>, and <code>formatter</code> is used to format the values if it is not <code>None</code>.  <p> <code> ReaderBase #  Inherits from <code>IBinaryParser</code> . A base class for a parser that deserializes bytes to Python objects.  <p> Descriptor Method: <code>deserialize</code> Additional Functions: </p>  FileIO(filepath):   Constructs the Reader with an <code>io.BufferedReader</code> stream context. Sets <code>read_bytes</code> to the <code>read</code> method of the stream.   BytestreamIO(initializer):   Constructs the Reader with an <code>io.BytesIO</code> stream context. Sets <code>read_bytes</code> to the <code>read</code> method of the stream.   _default_read_bytes(length):   The default function assigned to <code>read_bytes</code> when not in a stream context.   read_bytes(length):   A label that which the bytes-reading method of the stream is intended to be monkey-patched onto to avoid an extra attribute lookup and provide a unified API over different streams, i.e. <code>rw.read_bytes</code> vs. <code>rw._bytestream.read</code>.   peek_bytestring(length):   Reads up to <code>length</code> bytes from the stream without advancing the stream offset.  <p> <code> ValidatorBase #  Inherits from <code>IBinaryParser</code> . A base class for a parser that verifies whether two bytestreams deserialize to identical Python objects, and if not, to raises an error that originates at the first pair of read calls that deserialize to different values.  <p> Descriptor Method: <code>deserialize</code> Additional Functions: </p>  PrimaryFileIO(filepath):   Constructs the Validator's primary stream with an <code>io.BufferedReader</code> stream context.   PrimaryBytestreamIO(initializer):   Constructs the Validator's primary stream with an <code>io.BytesIO</code> stream context.   ReferenceFileIO(filepath):   Constructs the Validator's reference stream with an <code>io.BufferedReader</code> stream context.   ReferenceBytestreamIO(initializer):   Constructs the Validator's reference stream with an <code>io.BytesIO</code> stream context.   _default_read_bytes(length):   The default function assigned to <code>read_bytes</code> when not in a stream context.   read_bytes(length):   A label that which the bytes-reading method of the stream is intended to be monkey-patched onto to avoid an extra attribute lookup and provide a unified API over different streams, i.e. <code>rw.read_bytes</code> vs. <code>rw._bytestream.read</code>.   peek_bytestring(length):   Reads up to <code>length</code> bytes from the stream without advancing the stream offset.  <p> <code> WriterBase #  Inherits from <code>IBinaryParser</code> . A base class for a parser that serializes Python objects to bytes.  <p> Descriptor Method: <code>serialize</code> Additional Functions: </p>  FileIO(filepath):   Constructs the Writer with an <code>io.BufferedReader</code> stream context.   BytestreamIO(initializer):   Constructs the Writer with an <code>io.BytesIO</code> stream context.   write_bytes(length):   A label that which the bytes-reading method of the stream is intended to be monkey-patched onto to avoid an extra attribute lookup and provide a unified API over different streams, i.e. <code>rw.write_bytes</code> vs. <code>rw._bytestream.write</code>.  <p> <code> CounterBase #  Inherits from <code>IBinaryParser</code> . A base class for a parser that counts how many bytes a stream pointer will advance for each operator call.  <p> Descriptor Method: <code>count</code> Additional Functions: </p>  advance_offset(value):   Increments the stream offset by <code>value</code>.  <p> <code> OffsetCalculatorBase #  Inherits from <code>CounterBase</code> . A base class for a parser that counts how many bytes a stream pointer will advance for each operator call, and using this calculates the value of stream-offset variables within a structure.  <p> Descriptor Method: <code>calculate_offsets</code> if it exists. Else, <code>count</code></p>"},{"location":"04-framework/#streams","title":"Streams","text":"<p><code> ValidatorStream #  A stream used by ValidatorBase.   Functions:   set_primary_stream(stream):   Sets the primary stream of the ValidatorStream.   set_reference_stream(stream):   Sets the reference stream of the ValidatorStream.   close():   Closes the primary and reference streams, if they are not <code>None</code>.   seek(offset):   Seeks both the primary and reference streams to the given <code>offset</code>.   tell():   Returns the position of the primary stream, which should be synchronised with the reference stream.   read(count):   Reads <code>count</code> from both streams and raises a <code>ValidationError</code> if they are not equal.   write(data):   Raises <code>NotImplementedError</code>."},{"location":"05-futuredevelopment/","title":"Future Development","text":"<p>Given below is a list of potential features that should be considered (not necessarily accepted for implementation) before a v1.0 release.</p> <ul> <li>A way to extend arrays, and not just wipe them, when performing a deserialization.</li> <li>It may be necessary to conditionally erase <code>OffsetMarker</code> subscriptions if there are dynamic subscriptions during a parse operation and the binary object is parsed more than once.</li> <li>Create a 'terse-mode' struct declaration that generates a struct based on <code>ctypes</code> just by using class-level type annotations. This should automatically create <code>exbip_rw</code> and fully support mixed endianness. An incomplete test implementation is in the <code>dev</code> directory. This will likely require a parallel system of annotation types to the descriptor system.</li> <li>Expand the concept of 'contexts' beyond just endianness switching to create arbitrary contexts as mix-in classes.<ul> <li>Array reads would be significantly faster if <code>numpy</code> were used as a backend. Configuring whether primitive array descriptors use <code>struct</code> or <code>numpy</code> as a backend could be handled with a monkey-patched context, like endianness currently is.</li> <li>Switching between contiguous and non-contiguous reads could be a context option rather than requiring extra classes.</li> <li>This will eliminate the need for e.g. a distinction between <code>Reader</code> and <code>NonContiguousReader</code> and generalise these concepts without the need for explicit hardcoding.</li> </ul> </li> <li>Separate the concept of stream-handing from the parser classes themselves. A parser should interact with an <code>exbip</code> stream API that a stream-wrapper object must provide in order to wrap a stream. To prevent function call and attribute access overhead, this API should return member functions of the wrapped stream that are monkey-patched onto the parser within the confines of a context manager.</li> <li>Implement parsers that can read and write tagged schemas e.g. <code>json</code> and <code>XML</code> files.<ul> <li>This will required implementing 'tagged' versions of the primitive descriptors that also take a name, which is the label the descriptor is exported under.</li> </ul> </li> <li>Implement a parser that is an AST/code generator. The purpose of the parser is to create an AST that can be analysed in order to inline function calls and merge stream reads and writes into larger memory accesses, and can then be dumped to a Python file or compiled to an object as an optimised parser. This would essentially be a small optimising compiler acting on an AST generated by the parser.</li> </ul>"}]}